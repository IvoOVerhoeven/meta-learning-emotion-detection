{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "## Unified Emotion\n",
    "- ~~remove classes with limited data~~\n",
    "- ~~convert multiple labels to multiple examples with different labels~~\n",
    "- come up with better assignment scheme for train/valid/test splits\n",
    "\n",
    "## Go Emotions\n",
    "- Get rid of print when loading (low priority)\n",
    "\n",
    "## Manual Tokenizer\n",
    "- check if works for go emotion\n",
    "- ~~incorporate special tokens into huggingface tokenizer~~\n",
    "- ~~drop \"#SemST\" from ssec sentences~~\n",
    "- include goemotions cases for manual tokenizer\n",
    "\n",
    "## Dataloaders\n",
    "- loop Stratifiedloader for infinite sampling\n",
    "- ~~rewrite train script to use correct dataloaders~~\n",
    "- ~~allow Stratifiedloader to keep all classes (for supervised training)~~\n",
    "- ~~allow Stratifiedloader to keep map classes subset to internal mapping (for meta training)~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from data.utils.data_loader import StratifiedLoader, AdaptiveNKShotLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "## Unified Emotion\n",
    "\n",
    "Data source download: https://drive.google.com/file/d/1y7yjshepNRPhnh-Qz5MTRbnopGn7KzUm/view?usp=sharing\n",
    "Originally from: https://github.com/sarnthil/unify-emotion-datasets\n",
    "\n",
    "\n",
    "Klinger, R. & Bostan, L. (2018, August). An analysis of annotated corpora for emotion classification in text. In Proceedings of the 27th International Conference on Computational Linguistics (pp. 2104-2119)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     source   size         domain  classes  \\\n",
       "0             affectivetext    250      headlines        6   \n",
       "1               crowdflower  40000         tweets       14   \n",
       "2               dailydialog  13000  conversations        6   \n",
       "3           electoraltweets   4058         tweets        8   \n",
       "4                   emobank  10000      headlines        3   \n",
       "5                    emoint   7097         tweets        6   \n",
       "6             emotion-cause   2414     artificial        6   \n",
       "7   fb-valence-arousal-anon   2800       facebook        3   \n",
       "8         grounded_emotions   2500         tweets        2   \n",
       "9                      ssec   4868         tweets        8   \n",
       "10            tales-emotion  15302     fairytales        6   \n",
       "11                      tec  21051         tweets        7   \n",
       "\n",
       "                          special  \n",
       "0   non-discrete, multiple labels  \n",
       "1      includes no-emotions class  \n",
       "2      includes no-emotions class  \n",
       "3      includes no-emotions class  \n",
       "4                  VAD regression  \n",
       "5            annotated by experts  \n",
       "6                             N/A  \n",
       "7                   VA regression  \n",
       "8                             N/A  \n",
       "9    multiple labels per sentence  \n",
       "10     includes no-emotions class  \n",
       "11           annotated by experts  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>size</th>\n      <th>domain</th>\n      <th>classes</th>\n      <th>special</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>affectivetext</td>\n      <td>250</td>\n      <td>headlines</td>\n      <td>6</td>\n      <td>non-discrete, multiple labels</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>crowdflower</td>\n      <td>40000</td>\n      <td>tweets</td>\n      <td>14</td>\n      <td>includes no-emotions class</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dailydialog</td>\n      <td>13000</td>\n      <td>conversations</td>\n      <td>6</td>\n      <td>includes no-emotions class</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>electoraltweets</td>\n      <td>4058</td>\n      <td>tweets</td>\n      <td>8</td>\n      <td>includes no-emotions class</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>emobank</td>\n      <td>10000</td>\n      <td>headlines</td>\n      <td>3</td>\n      <td>VAD regression</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>emoint</td>\n      <td>7097</td>\n      <td>tweets</td>\n      <td>6</td>\n      <td>annotated by experts</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>emotion-cause</td>\n      <td>2414</td>\n      <td>artificial</td>\n      <td>6</td>\n      <td>N/A</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>fb-valence-arousal-anon</td>\n      <td>2800</td>\n      <td>facebook</td>\n      <td>3</td>\n      <td>VA regression</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>grounded_emotions</td>\n      <td>2500</td>\n      <td>tweets</td>\n      <td>2</td>\n      <td>N/A</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ssec</td>\n      <td>4868</td>\n      <td>tweets</td>\n      <td>8</td>\n      <td>multiple labels per sentence</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>tales-emotion</td>\n      <td>15302</td>\n      <td>fairytales</td>\n      <td>6</td>\n      <td>includes no-emotions class</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>tec</td>\n      <td>21051</td>\n      <td>tweets</td>\n      <td>7</td>\n      <td>annotated by experts</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data.unified_emotion import unified_emotion, unified_emotion_info\n",
    "\n",
    "pd.DataFrame(unified_emotion_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified = unified_emotion(\"./data/datasets/unified-dataset.jsonl\",\\\n",
    "    include=['crowdflower', 'dailydialog', 'electoraltweets', 'emoint', 'emotion-cause', 'grounded_emotions', 'ssec', 'tec'])\n",
    "\n",
    "unified.prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'grounded_emotions': 2585,\n",
       " 'ssec': 15444,\n",
       " 'crowdflower': 39821,\n",
       " 'dailydialog': 102805,\n",
       " 'emotion-cause': 1960,\n",
       " 'tec': 21043,\n",
       " 'emoint': 7090,\n",
       " 'electoraltweets': 3682}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "unified.lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "grounded_emotions\n",
      "1 RT @NICKinNOVA: @TuxcedoCat @RosLehtinen @Deidramayfair And that's without Medicaid expansion.\n",
      "0 #CEO &amp; CoFounder of #fes Parimal Naik and my #family @BallysVegas #team #creditsisters #credit #business #training https://t.co/6LCeW2PVb9\n",
      "\n",
      "ssec\n",
      "0 Stupid Feminists, the civilization you take for granted was built with the labour, blood sweat and tears of men. #SemST\n",
      "1 it's ironic that ppl will perform lifesaving therapies on animals to preserve their lives-but have staunch views in favor of #SemST\n",
      "2 #Weneedfeminism because Twitter has its very own misogynist harassment machine. #YesAllWomen #HeForShe #Feminist #SemST\n",
      "4 @TarheelKrystle @primatemachine Bowing down but not in a threatening way. B/c even bowing down can be triggering. #SemST\n",
      "5 Meredith giving Don crap was great,but HOLY CRAP PEGGY. No spoilers, but DAMN was it a great scene. #Peggy #MadMen #SemST\n",
      "6 You stick in the postcode on where you want to go and God will set the SatNav onhow to get there @davegilpin  @HopeCity #Destiny #SemST\n",
      "3 Contempo Qiwaamah:longer paid maternity leave,flexi working hours,on-site childcare,breastfeeding/breastpumping rooms,equal pay #SemST\n",
      "\n",
      "crowdflower\n",
      "5 I'll be grand....\n",
      "6 twitter is trippin right now with my pic\n",
      "3 i am SOOO mad...yesterday and today...SUCKED!!!so much!\n",
      "2 In the mood for shrimp scampi but I don't have vermouth.\n",
      "7 @ericajo42 @saynerd01 oh look! it's our new house in santa barbara! http://i41.tinypic.com/2hi2t4y.jpg  hahha i wish\n",
      "4 And happy star wars day\n",
      "0 @LaKia unfortunately noy   my life suck this year.\n",
      "\n",
      "dailydialog\n",
      "1 Really ? I've already opened it . Great ! And it is exactly after my fancy . Thank you very much .\n",
      "4 Well , he uses our VCR at home.But he's going to university in the Fall , so he needs his own VCR .\n",
      "3 Wow , this thing can haul .\n",
      "0 Why are you trying to rush me off the phone ?\n",
      "6 Oh , no ! Why ? I was looking forward to seeing you at my new place .\n",
      "5 I know . I feel bad about how much my car is adding to the pollution problem in this city .\n",
      "\n",
      "emotion-cause\n",
      "3 Perhaps elated behaviour has evolved in connection with a rise in the hierarchy , depressive behaviour with a fall . \n",
      "4 Whether the cries of anguish for 1991 alone are justified remains to be seen . \n",
      "0 Despite a flaring anger at this impertinence her eyes were drawn to the strong column of his throat as he swallowed . \n",
      "2 She was getting agitated . \n",
      "\n",
      "tec\n",
      "5 @LindseyC_13 @scottycampbell1 I figured it out!! Yay me! However, Grams was totally in the dark!\n",
      "4 My ipod is missing and I'm having serious separation anxiety\n",
      "3 We can guard our hearts so well that we're unable to let or #love reach us. Open &amp; trust. #inspire #quote\n",
      "1 Just ate at the shadiest Chinese buffet and now I'm paying for it. Eat that Kung pao cat meat....\n",
      "2 You should have the of not living up to one's repentance and breaking one's promise.\n",
      "0 Today I spent 15minutes getting ready and 15 ranting about my brother eating my advent calender.\n",
      "\n",
      "emoint\n",
      "0 Losing the will 2 live with @virginmedia business bb gone down on hold for 23 minutes &amp; whoever picked up cut me off  #NoWorkForMe\n",
      "1 But this is the internet age, so get mad out of any and all proportion and assume the terrible worst with little to no facts or knowledge.\n",
      "2 18' \\nMichael Carrick as struck the back of the net much to the delight of under pressure man u fanz and MOU...\n",
      "3 Brad Pitt being investigated for child abuse?! this all just got really #dark\n",
      "\n",
      "electoraltweets\n",
      "3 Not in our entire history has one of our ambassadors been murdered/anally raped untill.9/11/12 Obama partied w/jay-z,Kenya/kim, Hamas and\n",
      "0 @ErinBurnett @mittromney #beckyquick #warrenbuffet did warren buffet or Barack Obama give 30% to charity? If not, THEY should pay more tax.\n",
      "1 I side with @BarackObama on over 80 % of issues - he MUST get the chance to continue what he started! #obamabiden2012\n",
      "9 Cant wait to hear Obama's official acceptance speech tonight. Can it beat Clinton's &amp Michelle Obama's amazing speeches?! #DNC #4moreyears\n",
      "5 @andyrutledge #America needs this. God bless you sir! #election2012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in unified.lens.keys():\n",
    "    dataset = unified.datasets[k]['train']\n",
    "    trainloader = StratifiedLoader(dataset=dataset, device=torch.device('cpu'), k=1)\n",
    "    labels, text, _, _ = next(trainloader)\n",
    "    print(k)\n",
    "    for lab, sent in zip(labels, text):\n",
    "        print(lab, sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoEmotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No config specified, defaulting to: go_emotions/simplified\n",
      "Reusing dataset go_emotions (C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e)\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-07a134cc41feca48.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-d4dc7f7f3530d91a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-91437c43c19e5bec.arrow\n",
      "Removed go_emotions/embarrassment for too little data |train|=236, |test|=28\n",
      "Removed go_emotions/grief for too little data |train|=63, |test|=6\n",
      "Removed go_emotions/remorse for too little data |train|=401, |test|=45\n",
      "Removed go_emotions/relief for too little data |train|=88, |test|=7\n",
      "Removed go_emotions/pride for too little data |train|=51, |test|=7\n",
      "Removed go_emotions/nervousness for too little data |train|=98, |test|=14\n",
      "Removed a total of 6 classes and 1044 examples.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'go_emotions': 35447}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from data.go_emotions import go_emotions\n",
    "\n",
    "go_emotion = go_emotions(first_label_only=True)\n",
    "go_emotion.prep()\n",
    "\n",
    "go_emotion.lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No config specified, defaulting to: go_emotions/simplified\n",
      "Reusing dataset go_emotions (C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e)\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-07a134cc41feca48.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-d4dc7f7f3530d91a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-91437c43c19e5bec.arrow\n",
      "Removed go_emotions/embarrassment for too little data |train|=338, |test|=42\n",
      "Removed go_emotions/grief for too little data |train|=84, |test|=6\n",
      "Removed go_emotions/relief for too little data |train|=202, |test|=15\n",
      "Removed go_emotions/pride for too little data |train|=158, |test|=23\n",
      "Removed go_emotions/nervousness for too little data |train|=212, |test|=27\n",
      "Removed a total of 5 classes and 1107 examples.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'go_emotions': 43101}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "go_emotion = go_emotions(first_label_only=False)\n",
    "go_emotion.prep()\n",
    "\n",
    "go_emotion.lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "go_emotions\n2 It's a photograph with one of our greatest ever defender and captain. What's your problem?\n14 Not only do they make me cringe, I become angry because usually or is my nmother and her flying monkey daughter posting them. Just horrible.\n3 A machine would literally do a worse job than him.\n26 Wow, that crazy. Hope this doesn't happen again with you. Stay safe!\n15 Thanks bud, hope it’s a great one!\n8 I want to give my peers money, so they can fight for my rights and privilges- see the bus pass discussion in this thread. \n20 Oh that also would have been good. Plus, breaking pottery is so satisfying\n0 I can only focus on that pastry.\n6 Trash pick up vehicle driver. Uh.. a garbage man?\n1 This is what I'm currently dealing with and I'm glad I read this. Nice reminder thank you lol.\n4 This should be interesting...\n5 I am not having a great day either. Sending good vibes, hope you feel better :)\n22 For some reason I find these charming 🤷🏼‍♀️\n25 They usually do hoss, hence the steak and beer hard to be sad with good food. \n7 And what about selling your precious mind creation to someone that doesn't deserve it?\n17 Looks f*cking delicious\n18 I would love that, I kind of miss him\n13 Same here, she left the house we bought and is giving me till the end of the month. Woooo\n10 I understand why he's confused but [NAME] is the host of the show, he can't just leave\n9 Less is definitely more. That picture is just kind of forgettable old meme material. This post feels like old keyword stuffing.\n24 What a shame... she was passing really well too...\n11 I am an atheist and the idea of a god makes me disgusted and angry so I prefer not to be religious\n\n"
     ]
    }
   ],
   "source": [
    "for k in go_emotion.lens.keys():\n",
    "    dataset = go_emotion.datasets[k]['train']\n",
    "    trainloader = StratifiedLoader(dataset=dataset, device=torch.device('cpu'), k=1)\n",
    "    labels, text, _, _ = next(trainloader)\n",
    "    print(k)\n",
    "    for lab, sent in zip(labels, text):\n",
    "        print(lab, sent)\n",
    "    print()"
   ]
  },
  {
   "source": [
    "## Or just get everything at once with the MetaDataset method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Removed a total of 0 classes and 0 examples.\n",
      "No config specified, defaulting to: go_emotions/simplified\n",
      "Reusing dataset go_emotions (C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e)\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-07a134cc41feca48.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-d4dc7f7f3530d91a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-91437c43c19e5bec.arrow\n",
      "Removed go_emotions/embarrassment for too little data |train|=338, |test|=42\n",
      "Removed go_emotions/grief for too little data |train|=84, |test|=6\n",
      "Removed go_emotions/relief for too little data |train|=202, |test|=15\n",
      "Removed go_emotions/pride for too little data |train|=158, |test|=23\n",
      "Removed go_emotions/nervousness for too little data |train|=212, |test|=27\n",
      "Removed a total of 5 classes and 1107 examples.\n"
     ]
    }
   ],
   "source": [
    "from data.meta_dataset import MetaDataset\n",
    "\n",
    "dataset = MetaDataset(verbose=True, include=['go_emotions'])\n",
    "dataset.prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "go_emotions\n2 Oh look another shit call\n14 This must have been totally horrifying for you . I m glad to hear you found peace in the second birth .\n3 Tyl has got to be the most cringey , condescending thing I ve seen on Reddit in a while .\n26 OH MY GOD ! The PTA has disbanded ! Ahh ! Ahh ! AHHHHH jumps through window\n15 Nice to see someone actually posted something useful here . :D Thanks !\n8 Maybe one day but I am very cultured *\n20 Those not a part of the Austinfred bashing hive mind at the other sub will find themselves here tomorrow . :face_with_rolling_eyes: .\n0 I consider everyone attractive in their own way , so everyone is intimidating .\n6 Is this season 7 ? I do not remember it .\n1 Oh , I thought it was about the audio glitch at the end , lol Now I see\n4 My vanity point for my love for SS is that it has a lance lord .\n5 It may get you pregnant so just make sure you eat all her birth control before hand .\n22 I thought it was gen 2 that was affected by the dead batteries due to the internal clock ?\n25 This is sad but not cringe , I hope he finds someone to play with soon :sleepy_face:\n7 You mean you would not want to be a transhuman cyborg alien experiment ? That s not very cash money of you , my friend .\n17 its ok , glad i watched the show , i enjoyed it .\n18 Adorable little buggers .\n13 Excited for you guys to be there see you there <NAME> !\n10 I did not post this . I really do find it funny tho\n9 That s so disappointing . I m sorry that happened to you . It really is hard to make friends as an adult .\n24 This is a bad post and you should feel bad for being the author\n11 He was horrible\n\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from data.utils.data_loader import StratifiedLoader, AdaptiveNKShotLoader\n",
    "\n",
    "for k in dataset.lens.keys():\n",
    "    data_subset = dataset.datasets[k]['train']\n",
    "    trainloader = StratifiedLoader(dataset=data_subset,\\\n",
    "        device=torch.device('cpu'), k=1)\n",
    "    labels, text, _, _ = next(trainloader)\n",
    "    print(k)\n",
    "    for lab, sent in zip(labels, text):\n",
    "        print(lab, sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Damn youtube and outrage drama is super lucrative for reddit'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "dataset.datasets['go_emotions']['train'][0][0]['text'].encode('latin-1').decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tokenizer\n",
    "Here we define some rules for manually cleaning the imported data.\n",
    "Given this is all internet sourced, it's strongly recommended to define something at least.\n",
    "Current manual tokenizer will:\n",
    "- Correct the text encodings\n",
    "- Align contractions with BERT tokenizers\n",
    "- Handles emojis (using emoji package) and twitter handles\n",
    "- Deals with some edge cases where Spacy's tokenizer fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from data.utils.tokenizer import manual_tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.additional_special_tokens = [\"HTTPURL\", \"@USER\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['@belisawriter @Eliawriter fucking awesome!',\n",
       " 'Trump Wants Faster Growth. The Fed Isnt So Sure. - The New York Times https://t.co/uQulh2CYHk',\n",
       " '@kenzie_MFC @AubreyCyles_ what? How?',\n",
       " '@JihadiJew thank you! i will :)',\n",
       " 'RT @mysafela: Join us March 18th in San Pedro for a #SmokeAlarmAwarenessMonth community event in #SanPedro https://t.co/lESN9H2yCY',\n",
       " \"RT @CynthiaEriVo: Much respect @DanielKaluuya_ well said, heartbreakingly honest and for what it's worth I thought you were brilliant. http\",\n",
       " 'RT @SenWarren: .@realDonaldTrump, your Muslim ban is now 0 for 2 vs the Constitution. Stop fighting the rule of law and start fighting for',\n",
       " 'Trump adviser admits contact with Guccifer 2.0 during campaign - CBS News https://t.co/HD5M7bwx3F',\n",
       " 'RT @VAPolitical: Treason: Appearing on Russian state television, longtime Trump adviser Roger Stone pushes Trumps wiretap lie https://t.co',\n",
       " '@ReaganBattalion @Gavin_McInnes this guy needs a big Jewish foot up his ass! What sick twisted guy @Gavin_McInnes is...#Nazi #Bigot',\n",
       " 'If @DickDurbin lets one #weak corporate owned dem vote for #trumpcare then he is totally ineffective as minority wh https://t.co/LTIHr4jX5K',\n",
       " 'RT @RepDwightEvans: @RepJoeKennedy Thank You for getting on record how @HouseGOP #Trumpcare fails millions, including those w/ a mental ill',\n",
       " 'Sandhy Sondoro &amp; Bastian Lee Jones on Radio Paradiso 98.2 for Berlin oncert on 19th of March https://t.co/qOhQ5L4qOG',\n",
       " 'Just a #selfie #happy #atlanta #travel @ Atlanta, Georgia https://t.co/Ptx0AW6eGI',\n",
       " 'RT @JohnFugelsang: The greatest threat to America is not foreign terrorists, but domestic imbeciles.',\n",
       " \"RT @GeorgeTakei: Some criticize me for only tweeting about how I detest Trump. That's just not true. I often also tweet about how much othe\"]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "dataset = unified.datasets['grounded_emotions']['train']\n",
    "trainloader = StratifiedLoader(dataset=dataset, device=torch.device('cpu'), k=8)\n",
    "\n",
    "labels, text, _, _ = next(trainloader)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same, but now manually tokenized, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<USER> <USER> fucking awesome !',\n",
       " 'Trump Wants Faster Growth . The Fed Is nt So Sure . - The New York Times',\n",
       " '<USER> <USER> _ what ? How ?',\n",
       " '<USER> thank you ! i will :)',\n",
       " 'RT <USER> : Join us March 18th in San Pedro for a # SmokeAlarmAwarenessMonth community event in # SanPedro',\n",
       " 'RT <USER> : Much respect <USER> _ well said , heartbreakingly honest and for what it s worth I thought you were brilliant . http',\n",
       " 'RT <USER> : .<USER> , your Muslim ban is now 0 for 2 vs the Constitution . Stop fighting the rule of law and start fighting for',\n",
       " 'Trump adviser admits contact with Guccifer 2.0 during campaign - CBS News',\n",
       " 'RT <USER> : Treason : Appearing on Russian state television , longtime Trump adviser Roger Stone pushes Trumps wiretap lie',\n",
       " '<USER> <USER> this guy needs a big Jewish foot up his ass ! What sick twisted guy <USER> is ... #Nazi # Bigot',\n",
       " 'If <USER> lets one # weak corporate owned dem vote for # trumpcare then he is totally ineffective as minority wh',\n",
       " 'RT <USER> : <USER> Thank You for getting on record how <USER> # Trumpcare fails millions , including those w/ a mental ill',\n",
       " 'Sandhy Sondoro Bastian Lee Jones on Radio Paradiso 98.2 for Berlin oncert on 19th of March',\n",
       " 'Just a # selfie # happy # atlanta # travel @ Atlanta , Georgia',\n",
       " 'RT <USER> : The greatest threat to America is not foreign terrorists , but domestic imbeciles .',\n",
       " 'RT <USER> : Some criticize me for only tweeting about how I detest Trump . That s just not true . I often also tweet about how much othe']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "list(map(manual_tokenizer, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be easily slotted into the data loading process\n",
    "Does quite a lot longer though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use below if you additionally want to limit sentences to those that overlap well with BERT\n",
    "# Not recommended for initial training \n",
    "#unified.prep(text_tokenizer=manual_tokenizer, text_tokenizer_kwargs={'bert_vocab': tokenizer.vocab.keys(), 'OOV_cutoff' :0.5, 'verbose':True})\n",
    "\n",
    "unified.prep(text_tokenizer=manual_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Example data\n",
      "grounded_emotions\n",
      "sadness [CLS] rt < user > : amazing video showing how people rely on medicaid to * live * donald trump wants to slash the program by $ 880 billion. [SEP]\n",
      "joy [CLS] rt < user > : scared little hands! so - called potus # trumptraitor # russianpawn # p2 # tcot # resistance # trumprussia [SEP]\n",
      "\n",
      "ssec\n",
      "anger [CLS] < user > thank you, # progressive - minded pursuers of familial and societal dystopia. # democrats # liberals [SEP]\n",
      "disgust [CLS] < user > and where is < user >? nowhere to be seen - cause they only care about [SEP]\n",
      "fear [CLS] < user > your treatment of the press, amb stevens fam, and the intel of the american ppl is enough to send you to jail [SEP]\n",
      "sadness [CLS] i see more and more people each day question god s work and why he does things. if you believe, you should not have any questions. [SEP]\n",
      "surprise [CLS] fantastic emmet county dem meeting tonight in estherville. glad to meet some < user > supporters! [SEP]\n",
      "trust [CLS] pundits say jim webb facing uphill battle with hillary. really, it s with bernie for the anti - hillary vote. # jimwebb [SEP]\n",
      "joy [CLS] that nagging doubt you keep having about god? gods not testing u, it s your intellect trying to tell you your beliefs are bullshit. [SEP]\n",
      "\n",
      "crowdflower\n",
      "noemo [CLS] talking to nat [SEP]\n",
      "sadness [CLS] has a gym day and is hoping to enjoy the last friday of her twenties. [SEP]\n",
      "joy [CLS] fun day with boo. short but fun [SEP]\n",
      "fear [CLS] currently working on the sdp s prototype.... so much to do... [SEP]\n",
      "surprise [CLS] < user > yes i have read them many times. [SEP]\n",
      "love [CLS] trying to reorganize plans for tonight uggg hopefully it will still be lots o fun! [SEP]\n",
      "anger [CLS] i have to reupload the damn thing again [SEP]\n",
      "\n",
      "dailydialog\n",
      "disgust [CLS] you stink! [SEP]\n",
      "noemo [CLS] well, i finished my last final today. [SEP]\n",
      "joy [CLS] is it made of chocolate? [SEP]\n",
      "anger [CLS] stop talking like my mother. that s what she keeps saying. [SEP]\n",
      "surprise [CLS] what? why do we have to do that? [SEP]\n",
      "sadness [CLS] i m sorry, but we don't allow returns on sale items. [SEP]\n",
      "\n",
      "emotion-cause\n",
      "joy [CLS] from a starving stray it was transformed into a contented house cat. [SEP]\n",
      "sadness [CLS] ` the popular story is that he was so heartbroken he ran off to join a religious order, and has since been ordained. \" [SEP]\n",
      "anger [CLS] carlie and harvey feel bitter and resentful about the way they ve been treated. [SEP]\n",
      "fear [CLS] and you feel anxious about getting the first lot of seeds sown out of doors. [SEP]\n",
      "\n",
      "tec\n",
      "surprise [CLS] finished christmas shopping!! yippee! [SEP]\n",
      "sadness [CLS] having a hard time right now. lots of good thing it s time for sunday night # meditation [SEP]\n",
      "joy [CLS] meeting < user > on my birthday a few years back [SEP]\n",
      "disgust [CLS] ok guy on plane next to me... i get it. you have a nasty cold. but will you please cover your mouth?!! [SEP]\n",
      "fear [CLS] oh my god! bit slow but i ve only just realised the 16th is only a week away! [SEP]\n",
      "anger [CLS] is short, live it. is rare, grab it. is bad, dump it. # fear is awful, face it. # memories are sweet, cherish it. [SEP]\n",
      "\n",
      "emoint\n",
      "anger [CLS]..... wakes up and says have you tried changing her nappy? : pouting _ face : : oncoming _ fist : : medium - light _ skin _ tone :!!!! [SEP]\n",
      "fear [CLS] can only blame jose ere why would you give rojo another start after sunday, fucking disaster waiting to happen it did shocking header! [SEP]\n",
      "joy [CLS] the t. i / shawty lo beef is one of the more underrated ones in hip - hop history. chock - full of wit, bravado and hilarity. [SEP]\n",
      "sadness [CLS] < user > just heard back2back, guess that s why they call it the blues she s got the look, but i can only sing tickle sacks version [SEP]\n",
      "\n",
      "electoraltweets\n",
      "disgust [CLS] < user > not even with stretch arm strong arms? [SEP]\n",
      "anger [CLS] the conspiracy where obama s ama session was really just watson crowd - sourcing answers based on existing reddit content. # calledit [SEP]\n",
      "anticipation [CLS] < user > the number 1 issue should be the supreme court. # election2012 [SEP]\n",
      "trust [CLS] # areyoubetteroff i will be when < user > and < user > move into the white house. # gop2012 [SEP]\n",
      "joy [CLS] don't forget to register to vote! # natlvoterreg day! # vote # election2012 [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nExample data')\n",
    "for k in unified.lens.keys():\n",
    "    dataset = unified.datasets[k]['train']\n",
    "    trainloader = StratifiedLoader(dataset=dataset, device=torch.device('cpu'), k=1)\n",
    "\n",
    "    labels, text, _, _ = next(trainloader)\n",
    "    print(k)\n",
    "\n",
    "    label_map = {v: k for k, v in unified.label_map[k].items()}\n",
    "    tokenized_texts = list(map(tokenizer.decode, tokenizer(text)['input_ids']))\n",
    "    for txt, label in zip(tokenized_texts, labels):\n",
    "        print(label_map[label], txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go_emotion.prep(text_tokenizer=manual_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "## Dataset sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'tec'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "from data.utils.sampling import dataset_sampler\n",
    "\n",
    "source_name = dataset_sampler(unified, sampling_method='sqrt')\n",
    "source_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "Changed somewhat from last time. \n",
    "\n",
    "Now dataloaders must be generated manually using specific dataset (dict with labels as keys, lists of examples as values).\n",
    "\n",
    "Samples from data and returns **both** the support and query.\n",
    "\n",
    "Thus,\n",
    "\n",
    "IN: dataset\n",
    "\n",
    "OUT: support labels, support text, query labels, query text\n",
    "\n",
    "If Huggingface tokenizer is passed, text is full model input (attention masks, token types, etc.)\n",
    "\n",
    "Can be fed into model as,\n",
    "\n",
    "```\n",
    "model(**text)\n",
    "```\n",
    "\n",
    "### Stratified Sampling\n",
    "Traditional N-way k-shot, balanced across classes.\n",
    "\n",
    "Requires manually specifying k, which corresponds to batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from data.utils.data_loader import StratifiedLoader, AdaptiveNKShotLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = unified.datasets['ssec']['train']\n",
    "trainloader = StratifiedLoader(dataset=dataset, device=torch.device('cpu'), k=16)\n",
    "support_labels, support_text, query_labels, query_text = next(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({0: 16, 1: 16, 2: 16, 4: 16, 5: 16, 6: 16, 3: 16})"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "Counter(support_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({0: 16, 1: 16, 2: 16, 4: 16, 5: 16, 6: 16, 3: 16})"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "Counter(query_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while True:\n",
    "#    next(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive N-way k-shot\n",
    "Dataloader with adaptive/stochastic N-way, k-shot batches.\n",
    "\n",
    "Support set has random number of examples per class, although proportional to class size.\n",
    "\n",
    "Query set is always balanced.\n",
    "\n",
    "Not all classes are present if more than 5 classes are present in the dataset.\n",
    "\n",
    "Algorithm taken from:\n",
    "\n",
    "    Triantafillou et al. (2019). Meta-dataset: A dataset of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096.\n",
    "\n",
    "Steps are as follows:\n",
    "    \n",
    "1. Sample subset of classes (min 5, max all classes)\n",
    "    \n",
    "2. Define query set size (max 10 per class)\n",
    "    \n",
    "3. Define support set size (max 128 for all)\n",
    "    \n",
    "4. Fill support set with samples, stochastically proportional to support set size\n",
    "    \n",
    "5. Fill query set with remaining samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = unified.datasets['ssec']['train']\n",
    "trainloader = AdaptiveNKShotLoader(dataset=dataset, device=torch.device('cpu'), max_support_size=64)\n",
    "support_labels, support_text, query_labels, query_text = next(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({0: 38, 2: 15, 1: 10})"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "Counter(support_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({0: 21, 2: 21, 1: 21})"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "Counter(query_labels)"
   ]
  },
  {
   "source": [
    "Set `subset_classes=False` to retain all classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({4: 14, 2: 12, 6: 9, 1: 8, 0: 7, 3: 6, 5: 5}) 7\nCounter({0: 12, 4: 11, 1: 9, 6: 9, 3: 8, 2: 6, 5: 5}) 7\nCounter({0: 16, 4: 16, 1: 12, 3: 6, 5: 4, 6: 4, 2: 3}) 7\nCounter({4: 16, 3: 13, 0: 8, 1: 7, 2: 7, 6: 5, 5: 5}) 7\nCounter({4: 15, 0: 12, 2: 8, 6: 8, 1: 6, 5: 6, 3: 5}) 7\nCounter({0: 17, 6: 14, 4: 11, 3: 6, 2: 6, 1: 4, 5: 3}) 7\nCounter({6: 12, 3: 11, 1: 10, 2: 8, 4: 7, 0: 7, 5: 5}) 7\nCounter({0: 14, 3: 12, 4: 11, 6: 9, 2: 6, 1: 6, 5: 3}) 7\nCounter({4: 17, 3: 15, 0: 8, 1: 6, 6: 5, 2: 4, 5: 4}) 7\nCounter({0: 16, 6: 14, 3: 9, 4: 8, 2: 6, 1: 5, 5: 3}) 7\n"
     ]
    }
   ],
   "source": [
    "trainloader = AdaptiveNKShotLoader(dataset=dataset, device=torch.device('cpu'), max_support_size=64, subset_classes=False)\n",
    "\n",
    "for i in range(10):\n",
    "    support_labels, support_text, query_labels, query_text = next(trainloader)\n",
    "    print(Counter(support_labels), len(set(support_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({0: 33, 1: 30}) 2\nCounter({1: 9, 0: 6}) 2\nCounter({3: 18, 4: 15, 1: 12, 0: 9, 2: 8}) 5\nCounter({1: 18, 0: 17, 2: 13, 3: 10, 4: 4}) 5\nCounter({2: 22, 0: 21, 1: 19}) 3\nCounter({5: 12, 2: 12, 3: 7, 1: 7, 4: 6, 0: 6}) 6\nCounter({1: 36, 2: 14, 0: 13}) 3\nCounter({3: 22, 0: 20, 2: 9, 1: 7, 4: 4}) 5\nCounter({2: 18, 4: 14, 0: 12, 1: 10, 3: 9}) 5\nCounter({0: 27, 2: 23, 1: 12}) 3\n"
     ]
    }
   ],
   "source": [
    "trainloader = AdaptiveNKShotLoader(dataset=dataset, device=torch.device('cpu'), max_support_size=64, subset_classes=True)\n",
    "\n",
    "for i in range(10):\n",
    "    support_labels, support_text, query_labels, query_text = next(trainloader)\n",
    "    print(Counter(support_labels), len(set(support_labels)))"
   ]
  },
  {
   "source": [
    "Set `temp_map=False` to retain label definitions according to the dataset. \n",
    "\n",
    "Needs to be re-mapped to allow for generating one-hot vectors for loss computation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 1, 2]\n",
      "[0, 1, 2]\n",
      "[0, 1, 2]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "trainloader = AdaptiveNKShotLoader(dataset=dataset, device=torch.device('cpu'), max_support_size=64, temp_map=True)\n",
    "\n",
    "for i in range(10):\n",
    "    support_labels, support_text, query_labels, query_text = next(trainloader)\n",
    "    print(sorted(Counter(support_labels).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n[3, 6]\n[2, 3, 4, 5, 6]\n[1, 2]\n[0, 4, 6]\n[0, 1, 3, 4, 5, 6]\n[0, 1, 2, 4, 5, 6]\n[2, 5]\n[0, 1]\n[1, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "trainloader = AdaptiveNKShotLoader(dataset=dataset, device=torch.device('cpu'), max_support_size=64, temp_map=False)\n",
    "\n",
    "for i in range(10):\n",
    "    support_labels, support_text, query_labels, query_text = next(trainloader)\n",
    "    print(sorted(Counter(support_labels).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = unified.datasets['ssec']['train']\n",
    "\n",
    "trainloader = AdaptiveNKShotLoader(dataset=dataset, device=torch.device('cuda'), tokenizer=tokenizer, max_support_size=8, temp_map=True)\n",
    "for i in range(1000):\n",
    "    batch = next(trainloader)\n",
    "    support_labels, support_text, query_labels, query_text = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "grounded_emotions 1 211\ngrounded_emotions 0 306\nssec 0 1245\nssec 1 912\nssec 3 757\nssec 6 1205\nssec 4 1061\nssec 2 800\nssec 5 527\ncrowdflower 5 1893\ncrowdflower 6 1033\ncrowdflower 3 1854\ncrowdflower 2 1692\ncrowdflower 7 438\ncrowdflower 4 769\ncrowdflower 0 287\ndailydialog 1 71\ndailydialog 4 17115\ndailydialog 3 2577\ndailydialog 0 205\ndailydialog 6 365\ndailydialog 5 230\nemotion-cause 3 96\nemotion-cause 4 115\nemotion-cause 0 97\nemotion-cause 2 85\ntec 5 770\ntec 4 766\ntec 3 1647\ntec 1 153\ntec 2 564\ntec 0 311\nemoint 0 340\nemoint 1 449\nemoint 2 323\nemoint 3 307\nelectoraltweets 3 328\nelectoraltweets 0 114\nelectoraltweets 1 64\nelectoraltweets 9 162\nelectoraltweets 5 70\n"
     ]
    }
   ],
   "source": [
    "for task in unified.lens.keys():\n",
    "    subset = unified.datasets[task]['test']\n",
    "    for c in subset.keys():\n",
    "        print(task, c, len(subset[c]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd05807788318072d650ff7d3f12118168354b5d5c1b01788c0f8c7e03b8ca64544",
   "display_name": "Python 3.8.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}