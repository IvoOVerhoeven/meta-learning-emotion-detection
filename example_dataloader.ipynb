{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "## Unified Emotion\n",
    "- Remove samples with no-emotion class\n",
    "- Convert multiple labels to multiple examples with different labels\n",
    "- Come up with better assignment scheme for train/valid/test splits\n",
    "- Drop \"#SemST\" from ssec sentences\n",
    "\n",
    "## Go Emotions\n",
    "- Get rid of print when loading (low priority)\n",
    "- Include cases for manual tokenizer\n",
    "- Convert multiple labels to multiple examples with different labels (check with Luuk)\n",
    "\n",
    "## Manual Tokenizer\n",
    "- check if works for go emotion\n",
    "- incorporate special tokens into huggingface tokenizer\n",
    "\n",
    "## Dataloaders\n",
    "- loop Stratifiedloader for infinite sampling\n",
    "- ~~rewrite train script to use correct dataloaders~~\n",
    "- ~~allow Stratifiedloader to keep all classes (for supervised training)~~\n",
    "- ~~allow Stratifiedloader to keep map classes subset to internal mapping (for meta training)~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from data.utils.data_loader import StratifiedLoader, AdaptiveNKShotLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "## Unified Emotion\n",
    "\n",
    "Data source download: https://drive.google.com/file/d/1y7yjshepNRPhnh-Qz5MTRbnopGn7KzUm/view?usp=sharing\n",
    "Originally from: https://github.com/sarnthil/unify-emotion-datasets\n",
    "\n",
    "\n",
    "Klinger, R. & Bostan, L. (2018, August). An analysis of annotated corpora for emotion classification in text. In Proceedings of the 27th International Conference on Computational Linguistics (pp. 2104-2119)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     source   size         domain  classes  \\\n",
       "0             affectivetext    250      headlines        6   \n",
       "1               crowdflower  40000         tweets       14   \n",
       "2               dailydialog  13000  conversations        6   \n",
       "3           electoraltweets   4058         tweets        8   \n",
       "4                   emobank  10000      headlines        3   \n",
       "5                    emoint   7097         tweets        6   \n",
       "6             emotion-cause   2414     artificial        6   \n",
       "7   fb-valence-arousal-anon   2800       facebook        3   \n",
       "8         grounded_emotions   2500         tweets        2   \n",
       "9                      ssec   4868         tweets        8   \n",
       "10            tales-emotion  15302     fairytales        6   \n",
       "11                      tec  21051         tweets        7   \n",
       "\n",
       "                          special  \n",
       "0   non-discrete, multiple labels  \n",
       "1      includes no-emotions class  \n",
       "2      includes no-emotions class  \n",
       "3      includes no-emotions class  \n",
       "4                  VAD regression  \n",
       "5            annotated by experts  \n",
       "6                             N/A  \n",
       "7                   VA regression  \n",
       "8                             N/A  \n",
       "9    multiple labels per sentence  \n",
       "10     includes no-emotions class  \n",
       "11           annotated by experts  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>size</th>\n      <th>domain</th>\n      <th>classes</th>\n      <th>special</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>affectivetext</td>\n      <td>250</td>\n      <td>headlines</td>\n      <td>6</td>\n      <td>non-discrete, multiple labels</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>crowdflower</td>\n      <td>40000</td>\n      <td>tweets</td>\n      <td>14</td>\n      <td>includes no-emotions class</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dailydialog</td>\n      <td>13000</td>\n      <td>conversations</td>\n      <td>6</td>\n      <td>includes no-emotions class</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>electoraltweets</td>\n      <td>4058</td>\n      <td>tweets</td>\n      <td>8</td>\n      <td>includes no-emotions class</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>emobank</td>\n      <td>10000</td>\n      <td>headlines</td>\n      <td>3</td>\n      <td>VAD regression</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>emoint</td>\n      <td>7097</td>\n      <td>tweets</td>\n      <td>6</td>\n      <td>annotated by experts</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>emotion-cause</td>\n      <td>2414</td>\n      <td>artificial</td>\n      <td>6</td>\n      <td>N/A</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>fb-valence-arousal-anon</td>\n      <td>2800</td>\n      <td>facebook</td>\n      <td>3</td>\n      <td>VA regression</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>grounded_emotions</td>\n      <td>2500</td>\n      <td>tweets</td>\n      <td>2</td>\n      <td>N/A</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ssec</td>\n      <td>4868</td>\n      <td>tweets</td>\n      <td>8</td>\n      <td>multiple labels per sentence</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>tales-emotion</td>\n      <td>15302</td>\n      <td>fairytales</td>\n      <td>6</td>\n      <td>includes no-emotions class</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>tec</td>\n      <td>21051</td>\n      <td>tweets</td>\n      <td>7</td>\n      <td>annotated by experts</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data.unified_emotion import unified_emotion, unified_emotion_info\n",
    "\n",
    "pd.DataFrame(unified_emotion_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified = unified_emotion(\"./data/datasets/unified-dataset.jsonl\",\\\n",
    "    include=['crowdflower', 'dailydialog', 'electoraltweets', 'emoint', 'emotion-cause', 'grounded_emotions', 'ssec', 'tec'])\n",
    "\n",
    "unified.prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'grounded_emotions': 2585,\n",
       " 'ssec': 4868,\n",
       " 'crowdflower': 40000,\n",
       " 'dailydialog': 102979,\n",
       " 'emotion-cause': 2414,\n",
       " 'tec': 21051,\n",
       " 'emoint': 7102,\n",
       " 'electoraltweets': 4056}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "unified.lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "grounded_emotions\n",
      "1 @TRobinsonNewEra @eaaknighterrant @geertwilderspvv in Los Angeles we riot when the Lakers win! Big time! Bunch o' amateurs in Holland.\n",
      "0 Forever Goalsâ¡#i2i20proofâ¡Love Each Other â¡#BlackLove #blacklovers #loversandfriendsâ¦ https://t.co/XrlkpAVBpE\n",
      "\n",
      "ssec\n",
      "0 @madisonfletch_ but it's ok. at least it's only yo your hand that smells like dick #SemST\n",
      "2 It's hotter in Oregon then it is in California right now. #SemST\n",
      "3 @Anomaly100 I'm confident that will piss off a few white Anglo-Saxon Protestants!  Keep talking Jeb. #SemST\n",
      "1 @SCOTUSblog My @TheGoodGodAbove, is it 1973 already? #SemST\n",
      "6 John Watterson is attending IPPC meeting to help shape the future of international greenhouse gas emission estimation methods #SemST\n",
      "4 Why is there no #CaptainPlanet movie??? Rt if u want one. #environment #planet #earth #SemST\n",
      "5 The most revealing part about Hillary's released emails is that they're pretty much as boring as mine. #nofriends #SemST\n",
      "\n",
      "crowdflower\n",
      "5 @djhazzard whoa... if the alcohol won't get you buzzed... the price will\n",
      "6 i miss my boo  on another note im soready for this game to come on tonight...fox grill anyone???!!\n",
      "3 @JonathanRKnight BTW I STILL can't believe how Awesome the NEWJABBAKIDZ performance was...U in the masks..I screamed at my pc\n",
      "2 waiting to go to 4th period to get the final over with. omgg im sooo gonna fail\n",
      "7 @esmebella Kk, I just had 888 followers like a minute ago\n",
      "4 @DesktopGoldfish Yay, three followers! Good to know more than one person in this big wide world likes fishies.\n",
      "0 Lol..2 people in falcon..and boo one of them being becca\n",
      "1 @Candyland3 thats terrible.\n",
      "\n",
      "dailydialog\n",
      "1 Actually , what bothers me is the violence . There are far too many detective and police shows .\n",
      "4 Have you heard what happened to him ?\n",
      "3 That â s it ? Thank you very much .\n",
      "0 Hi , Gary , what happened ? I was waiting for you at the theater . Why didn â t you meet me there ?\n",
      "6 Really ? In New Berlin ?\n",
      "5 I â m sorry , Jim .\n",
      "2 Now he's looking over here ! He's coming !\n",
      "\n",
      "emotion-cause\n",
      "3 The bouncer 's glee , the masculine grunts in the background of ` Quite right , too \" , were a timely reminder of surviving and thriving misogyny , and of the petty power struggle it engenders . \n",
      "4 ` God give me strength , \" he yelled , instantly turning his attention back to the younger man who stood before him , and whose woebegone expression would have been comical were it not so pitiful . \n",
      "5 She seemed flabbergasted , but rallied and asked me if I would look in at the Gray Mare in Kilburn and say ` hello \" to her son Joe Kelly who worked there . \n",
      "1 They can be happy and cheerful but they can also get fed-up , frustrated , angry , disappointed and hurt inside -- just like real people . \n",
      "0 ` I 'm so angry our little boy is dead because someone wanted to drive fast . \" \n",
      "2 Eyewitnesses told of the horror as shrapnel rained down on shoppers from two devices placed in cast-litter bins less than ten yards apart . \n",
      "\n",
      "tec\n",
      "5 Following @elizadushku and she is pretty awesome...her tweets surprise me.\n",
      "4 My cell phone has low battery I don't know where my charger is\n",
      "3 @DCRUColin as I only have a PS3 it's the only thing I can play it on but I would prefer that BC2 disc\n",
      "1 Hate some people. It amazes me how many faces people can have.\n",
      "2 To @AprinoBerhitu\n",
      "0 Hate when you tell someone your going to call them and they &quot;go to sleep early&quot; #phonecalls\n",
      "\n",
      "emoint\n",
      "0 @canada4trumpnow @donlemon he has BAD TEMPERAMENT #tantrums #HissyFits\n",
      "1 we shiver in the pause between words \\nabandonment still fresh upon the tips of our tongues\n",
      "2 #FF @ManihaAamir @adrence Keep on #smiling â¦Â !\n",
      "3 Cuddling literally kills depression, relieves anxiety, and strengthens the immune system.\n",
      "\n",
      "electoraltweets\n",
      "3 #Obama admin uses #China's news Xinhua to further their own #communist propaganda. Birds of a feather hugh? Nice move comrade O. #Socialist\n",
      "0 Hmm @NJGovChristie bad mouths @JerryBrownGov yet Meg Whitman has record losses/layoffs @HP. She can't run a company let alone my state #GOP\n",
      "1 i have so much to tell Barack Obama and i'm finally gonna have a chance to tell him.\n",
      "9 Just ordered a @MittRomney shirt and car decal! I can't wait to wear it loud and proud! #Mitt2012 #RomneyRyan2012\n",
      "2 Romney can't have a chance, can he? To a swede, he's pretty surreal. #election\n",
      "5 @tmatsamas94 Okay. No more JB jokes. but it looks like those pictures you sent me. Hm.... Anyways... GO ROMNEY! #RomneyRyan2012\n",
      "8 Saw President #41 and #43 today at @rydercup, even saw Rev Jesse Jackson, guess #democrats don't support the biggest golf event in the land!\n",
      "4 I had the worst dream ever last night...I was in a voting booth and I picked mitt romney!!! Aaahhhhh. #mittromney #republicans\n",
      "7 Realizing that politics and Facebook friendships don't mix this election season. #election2012 #notupforthebullying\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in unified.lens.keys():\n",
    "    dataset = unified.datasets[k]['train']\n",
    "    trainloader = StratifiedLoader(dataset=dataset, device=torch.device('cpu'), k=1)\n",
    "    labels, text, _, _ = next(trainloader)\n",
    "    print(k)\n",
    "    for lab, sent in zip(labels, text):\n",
    "        print(lab, sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoEmotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No config specified, defaulting to: go_emotions/simplified\n",
      "Reusing dataset go_emotions (C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e)\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-07a134cc41feca48.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-d4dc7f7f3530d91a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-91437c43c19e5bec.arrow\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'go_emotions': 36491}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from data.go_emotions import go_emotions\n",
    "\n",
    "go_emotion = go_emotions(first_label_only=True)\n",
    "go_emotion.prep()\n",
    "\n",
    "go_emotion.lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No config specified, defaulting to: go_emotions/simplified\n",
      "Reusing dataset go_emotions (C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e)\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-07a134cc41feca48.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-d4dc7f7f3530d91a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ivoon\\.cache\\huggingface\\datasets\\go_emotions\\simplified\\0.0.0\\ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e\\cache-91437c43c19e5bec.arrow\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'go_emotions': 44208}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "go_emotion = go_emotions(first_label_only=False)\n",
    "go_emotion.prep()\n",
    "\n",
    "go_emotion.lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "go_emotions\n2 Bitch yes!\n14 Must be a terrible swim team. The only thing that your neck beard does is accentuates your double chin.\n3 So far she seems pretty decent but I'm side-eyeing the hell out of him. His conduct at the restaurant was so bizarre and creepy.\n26 This is not a comedy subreddit. It is a subreddit about the unexpected.\n15 >Relax ms bi weeb who leans left haha... thanks i love it\n8 I wish I could get the entire interview but it's from a SiriusXM Radio Show\n20 Problem is we don't have much else of value. [NAME], [NAME], [NAME]. We have to hope [NAME] likes THJ.\n0 Amazing!! Congrats!\n6 I can't tell if the is comfortable in that outfit\n1 Lol you sweet summer child. I wish I weren't so cynical and had hope like this.\n4 I’d like to add that it’s British slang. We also use ‘kiddy fiddler’.\n5 Oh sorry dude, I was just looking for the WiFi password. [NAME], but I hope you stay safe\n12 I kept hoping [NAME] would walk out and shame DG.\n22 Can't play defence when everything is call and on the other end the Bulls haven't gotten a call since early in the quarter ...\n16 although he is best girl, he can never love :( he litterally doesn't have a heart\n25 Sometimes it even feels pathetic which I know I should be trying not to be so hard on myself but, yknow, evidence is hard to ignore\n7 Can these guys just pick a team in the AL and go there??\n17 Great. So increase prices 20% overall, give the waiters the extra money. I'm happy with that. 20% is a great tip.\n18 Nah I’m good. I like my parents and their food.\n13 Awesome! Congratulations. That is a nice investment.\n10 I made a post about a month ago saying [NAME] was trash and it got over 100 downvotes\n9 The same comedy bit [NAME] has disavowed because it's used almost exclusively in circumstances like this? It's an awful and weak explantation.\n24 That is utterly horrifying. I'm so sorry that your kindergarten teacher was such a heartless old bat.\n11 It's weird that actual socialists will say that politics has moved far to the right.\n23 Now let's sit down, relax, and have a nice cool glass of turnip juice.\n21 Wow OP sounds like you're really skilled at opening your legs, you're parents should be proud of you, STI and all.\n19 That was brutal as hell. Loved it\n\n"
     ]
    }
   ],
   "source": [
    "for k in go_emotion.lens.keys():\n",
    "    dataset = go_emotion.datasets[k]['train']\n",
    "    trainloader = StratifiedLoader(dataset=dataset, device=torch.device('cpu'), k=1)\n",
    "    labels, text, _, _ = next(trainloader)\n",
    "    print(k)\n",
    "    for lab, sent in zip(labels, text):\n",
    "        print(lab, sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tokenizer\n",
    "Here we define some rules for manually cleaning the imported data.\n",
    "Given this is all internet sourced, it's strongly recommended to define something at least.\n",
    "Current manual tokenizer will:\n",
    "- Correct the text encodings\n",
    "- Align contractions with BERT tokenizers\n",
    "- Handles emojis (using emoji package) and twitter handles\n",
    "- Deals with some edge cases where Spacy's tokenizer fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from data.utils.tokenizer import manual_tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.additional_special_tokens = [\"HTTPURL\", \"@USER\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"She disagreed with his club choice probably:Henrik Stenson's 2-year-old daughter runs on-course to Dad mid-round https://t.co/U6h8B4Sikr\",\n",
       " 'This is my view on Sunday! WooHoo. https://t.co/0bLiGHNWkz',\n",
       " 'The Crash caused by GOP Policies killed housing &amp; put banking in jeopardy! #Trumpism tests our Judicial System &amp; ouâ\\x80¦ https://t.co/74lrq6Ko4k',\n",
       " \"This moment. #morning #tagthecat laying on my chest and throat. He's #happy I'm happy. #cat @â\\x80¦ https://t.co/leje0GLjGN\",\n",
       " \"@mikkwallace Communications is the ONLY way to work 2gether. Communication makes everyone become US.We are US  (the world) &amp; I'm not stppng\",\n",
       " 'This is all over my Facebook feed. Seems fair to me. @TheDemocrats @SenateGOP @HouseGOP #trumpcare https://t.co/dVPo1igjfb',\n",
       " '@dcb97 yep m totally jealous!',\n",
       " 'RT @shakeology: Sunday plans: Shakeology and a stretch with Beachbody Yoga Super Yogi @EliseJoanFitnes  #Shakeology #BeachbodyYoga https://â\\x80¦',\n",
       " 'RT @puppymnkey: I had a premonition that the Dow dropped below 11,000. Very vivid.',\n",
       " 'A #fish filled #birthday #lunch the #eating tour continues at Garcias. Perfect! #oysters #conchâ\\x80¦ https://t.co/TJyr0a6Ld5',\n",
       " 'RT @RealMuckmaker: Trump Is Officially In Big Trouble Over Waste, Fraud, &amp; Corruption (DETAILS) https://t.co/6zqLkY4HhZ via @Bipartisan Repâ\\x80¦',\n",
       " \"Alice's face thinking about the weather on Thursday. alicethatcj #smile #happy #fuji #fujifilmâ\\x80¦ https://t.co/k3Fu88D7OS\",\n",
       " \"Why am I #happy?'Cause I'm at my true home away from home. 'Cause I'm at my favorite outdoorâ\\x80¦ https://t.co/AbExU3hyNH\",\n",
       " 'RT @tonyposnanski: Just to remind you...Steve King- Racist and horrible personStephen King- Writer and awesome personSteve Bannon- Prâ\\x80¦',\n",
       " '@AndrewBuncombehttpâ\\x80¦',\n",
       " 'RT @Amy_Siskind: Monster!!! https://t.co/xvpNfysbR4']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "dataset = unified.datasets['grounded_emotions']['train']\n",
    "trainloader = StratifiedLoader(dataset=dataset, device=torch.device('cpu'), k=8)\n",
    "\n",
    "labels, text, _, _ = next(trainloader)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same, but now manually tokenized, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['she disagreed with his club choice probably : henrik stenson s 2 - year - old daughter runs on - course to dad mid - round HTTPURL',\n",
       " 'this is my view on sunday ! woohoo . HTTPURL',\n",
       " 'the crash caused by gop policies killed housing put banking in jeopardy ! # trumpism tests our judicial system ou ... HTTPURL',\n",
       " 'this moment . # morning # tagthecat laying on my chest and throat . he s # happy i am happy . # cat @ ... HTTPURL',\n",
       " '@USER communications is the only way to work 2gether . communication makes everyone become us . we are us ( the world ) i am not stppng',\n",
       " 'this is all over my facebook feed . seems fair to me . @USER @USER @USER # trumpcare HTTPURL',\n",
       " '@USER yep m totally jealous !',\n",
       " 'rt @USER : sunday plans : shakeology and a stretch with beachbody yoga super yogi @USER # shakeology # beachbodyyoga HTTPURL ...',\n",
       " 'rt @USER : i had a premonition that the dow dropped below 11 , 000 . very vivid .',\n",
       " 'a # fish filled # birthday # lunch the # eating tour continues at garcias . perfect ! # oysters # conch ... HTTPURL',\n",
       " 'rt @USER : trump is officially in big trouble over waste , fraud , corruption ( details ) HTTPURL via @USER rep ...',\n",
       " 'alice s face thinking about the weather on thursday . alicethatcj # smile # happy # fuji # fujifilm ... HTTPURL',\n",
       " \"why am i # happy ?' cause i am at my true home away from home . because i am at my favorite outdoor ... HTTPURL\",\n",
       " 'rt @USER : just to remind you ... steve king - racist and horrible personstephen king - writer and awesome personsteve bannon - pr ...',\n",
       " '@USER ...',\n",
       " 'rt @USER : monster ! ! ! HTTPURL']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "list(map(manual_tokenizer, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be easily slotted into the data loading process\n",
    "Does quite a lot longer though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n",
      "Removed sentence for bad encoding.\n"
     ]
    }
   ],
   "source": [
    "# Use below if you additionally want to limit sentences to those that overlap well with BERT\n",
    "# Not recommended for initial training \n",
    "#unified.prep(text_tokenizer=manual_tokenizer, text_tokenizer_kwargs={'bert_vocab': tokenizer.vocab.keys(), 'OOV_cutoff' :0.5, 'verbose':True})\n",
    "\n",
    "unified.prep(text_tokenizer=manual_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Example data\n",
      "grounded_emotions\n",
      "sadness [CLS] rt @ user : it s rafa timeeee!!!! vamos rafa, you got thisss!!!! : red _ heart : : red _ heart : : red _ heart : : flexed _ biceps : : flexed _ biceps : httpurl [SEP]\n",
      "joy [CLS] rt @ user : glenn, you know i love you. but the joke was how easy trump can get away with being a lying conspiracy theorist. maybe not... [SEP]\n",
      "\n",
      "ssec\n",
      "anger [CLS] @ user i don't trust you performing your \" science - based medicine \" on children. i don't think it s science, or why reiterate? # semst [SEP]\n",
      "fear [CLS] the only esteem that will not abandon us is the esteem given to us by jesus. ~ scott sauls @ user # esteem # semst [SEP]\n",
      "joy [CLS] if i ever meet hillary clinton i will have died and then come back to life b / c i will be so happy # idol # semst [SEP]\n",
      "disgust [CLS] she has a brain, a heart, and her own unique dna, not her mother s. she is alive and human. please don't kill her. # semst [SEP]\n",
      "trust [CLS] lk 6 : 37 kjv judge not, and ye shall not be judged : condemn not, and ye shall not be condemned... # semst [SEP]\n",
      "sadness [CLS] how can there be too many # children? that is like saying there are too many # flowers - mother teresa # prolifeyouth # semst [SEP]\n",
      "surprise [CLS] serious question for my atheist libertarians : how can rights exist without god? # christianlibertarian # semst [SEP]\n",
      "\n",
      "crowdflower\n",
      "noemo [CLS] after show at our house rocked! saying goodbye soon [SEP]\n",
      "sadness [CLS] town. uncle and gabriel are coming in. i miss gabe tempted to ask if i can go back to baton rouge with and stay with other side of fam... [SEP]\n",
      "joy [CLS] @ user absolutely! just have a good backup for romo for december. [SEP]\n",
      "fear [CLS] i know i have no clean clothes either. and the washer s in the kitchen damn you # kitchenfire [SEP]\n",
      "surprise [CLS] gogowww how come we can not have deals like that from houston? [SEP]\n",
      "love [CLS] @ user _ httpurl - i liked old ruby best. [SEP]\n",
      "anger [CLS] wtf facebook just cleared out my whole survey and i was on the last q, this night gets better and better what else is next? [SEP]\n",
      "disgust [CLS] i am so tired [SEP]\n",
      "\n",
      "dailydialog\n",
      "disgust [CLS] i think it is the worst one i have ever seen. [SEP]\n",
      "noemo [CLS] yes, but there is a five dollars delivery charge. [SEP]\n",
      "joy [CLS] well... how about friday then? [SEP]\n",
      "anger [CLS] i am most grateful. everyone in our class enjoyed it. [SEP]\n",
      "surprise [CLS] a physics class about the creation of the universe? that s some pretty unscientific language there. sounds more religious to me. [SEP]\n",
      "sadness [CLS] i am sorry for that. we will improve the dishes next time. [SEP]\n",
      "fear [CLS] promise that you will not get angry. [SEP]\n",
      "\n",
      "emotion-cause\n",
      "joy [CLS] i was happy that i was having my own baby but sometimes i used to cry for nothing and i was easily upset. [SEP]\n",
      "sadness [CLS] i couldn't bear the mournful look on his face. [SEP]\n",
      "surprise [CLS] emily could see his puzzlement. [SEP]\n",
      "disgust [CLS] she closed her eyes to hide her revulsion as he pressed his open mouth against hers and began fumbling with the buttons of her bush shirt. [SEP]\n",
      "anger [CLS] for the opposition, sam galbraith said he was particularly angry that the government was reneging on the ballot subsidy. [SEP]\n",
      "fear [CLS] it was just like rambo, said one of the scores of horrified students who fled from the university after the massacre. [SEP]\n",
      "\n",
      "tec\n",
      "surprise [CLS] rule of thumb when boozin with a group of buddies, never be the last to pay the bill [SEP]\n",
      "sadness [CLS] stupid jew book [SEP]\n",
      "joy [CLS] six page paper, presentation, and pike banner all due tomorrow # busybusy [SEP]\n",
      "disgust [CLS] ok why the fuck is this girl getting into detail about this shit [SEP]\n",
      "fear [CLS] there is no shame in being afraid. we are all afraid of something. [SEP]\n",
      "anger [CLS] i hate chain messages.. send on to 10 people to bring you luck!! & if!!! [SEP]\n",
      "\n",
      "emoint\n",
      "anger [CLS] @ user @ user : ^ ) well cucks among her ranks agree equally about their outrage of black youths shot and harambe. [SEP]\n",
      "fear [CLS] @ user wow, did you get a really good buy on a bunch of 70 s movies? there were bad then and are worse now! # awful [SEP]\n",
      "joy [CLS] by the way, i am wearing the smile you gave me today. \\ n # you # me [SEP]\n",
      "sadness [CLS] patti seems so sad. she stamped and ran behind the sofa. we will have to give her plenty of love and affection... more than usual. [SEP]\n",
      "\n",
      "electoraltweets\n",
      "disgust [CLS] nicki minaj is a dumb bitch buh she voting for romney # gtfoh [SEP]\n",
      "anger [CLS] @ user if you have not seen the youtube video of obama out of his own mouth admitting he is a muslim please watch it [SEP]\n",
      "anticipation [CLS] don't know why but i am really looking forward to # obama v. # romney tomorrow night. can not wait for the # debate. [SEP]\n",
      "trust [CLS] i have so much to tell barack obama and i am finally going to have a chance to tell him. [SEP]\n",
      "confusion [CLS] @ user why is mitt romney being mormon an but harry reid, obama right hand man, being mormon a non - issue? [SEP]\n",
      "joy [CLS] we toke care of business in 2008 now it s time to do the same in 2012!! # 4moreyears # obamaordie? obama accomplishments? [SEP]\n",
      "surprise [CLS] # republicans are comedic gold this election year lol # obama2012 [SEP]\n",
      "fear [CLS] # romney is running for # president, thinking that half of the country is a troop of slackers and opportunists. # god save the world from him [SEP]\n",
      "sadness [CLS] realizing that politics and facebook friendships don't mix this election season. # election2012 # notupforthebullying [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nExample data')\n",
    "for k in unified.lens.keys():\n",
    "    dataset = unified.datasets[k]['train']\n",
    "    trainloader = StratifiedLoader(dataset=dataset, device=torch.device('cpu'), k=1)\n",
    "\n",
    "    labels, text, _, _ = next(trainloader)\n",
    "    print(k)\n",
    "\n",
    "    label_map = {v: k for k, v in unified.label_map[k].items()}\n",
    "    tokenized_texts = list(map(tokenizer.decode, tokenizer(text)['input_ids']))\n",
    "    for txt, label in zip(tokenized_texts, labels):\n",
    "        print(label_map[label], txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go_emotion.prep(text_tokenizer=manual_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "## Dataset sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'dailydialog'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "from data.utils.sampling import dataset_sampler\n",
    "\n",
    "source_name = dataset_sampler(unified, sampling_method='sqrt')\n",
    "source_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "Changed somewhat from last time. \n",
    "\n",
    "Now dataloaders must be generated manually using specific dataset (dict with labels as keys, lists of examples as values).\n",
    "\n",
    "Samples from data and returns **both** the support and query.\n",
    "\n",
    "Thus,\n",
    "\n",
    "IN: dataset\n",
    "\n",
    "OUT: support labels, support text, query labels, query text\n",
    "\n",
    "If Huggingface tokenizer is passed, text is full model input (attention masks, token types, etc.)\n",
    "\n",
    "Can be fed into model as,\n",
    "\n",
    "```\n",
    "model(**text)\n",
    "```\n",
    "\n",
    "### Stratified Sampling\n",
    "Traditional N-way k-shot, balanced across classes.\n",
    "\n",
    "Requires manually specifying k, which corresponds to batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from data.utils.data_loader import StratifiedLoader, AdaptiveNKShotLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = unified.datasets['ssec']['train']\n",
    "trainloader = StratifiedLoader(dataset=dataset, device=torch.device('cpu'), k=16)\n",
    "support_labels, support_text, query_labels, query_text = next(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({0: 16, 2: 16, 3: 16, 1: 16, 6: 16, 4: 16, 5: 16})"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "Counter(support_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({0: 16, 2: 16, 3: 16, 1: 16, 6: 16, 4: 16, 5: 15})"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "Counter(query_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while True:\n",
    "#    next(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive N-way k-shot\n",
    "Dataloader with adaptive/stochastic N-way, k-shot batches.\n",
    "\n",
    "Support set has random number of examples per class, although proportional to class size.\n",
    "\n",
    "Query set is always balanced.\n",
    "\n",
    "Not all classes are present if more than 5 classes are present in the dataset.\n",
    "\n",
    "Algorithm taken from:\n",
    "\n",
    "    Triantafillou et al. (2019). Meta-dataset: A dataset of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096.\n",
    "\n",
    "Steps are as follows:\n",
    "    \n",
    "1. Sample subset of classes (min 5, max all classes)\n",
    "    \n",
    "2. Define query set size (max 10 per class)\n",
    "    \n",
    "3. Define support set size (max 128 for all)\n",
    "    \n",
    "4. Fill support set with samples, stochastically proportional to support set size\n",
    "    \n",
    "5. Fill query set with remaining samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = unified.datasets['ssec']['train']\n",
    "trainloader = AdaptiveNKShotLoader(dataset=dataset, device=torch.device('cpu'), max_support_size=64)\n",
    "support_labels, support_text, query_labels, query_text = next(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({3: 1, 0: 34, 1: 2, 4: 1, 2: 6})"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "Counter(support_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({3: 10, 0: 10, 1: 10, 4: 10, 2: 10})"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "Counter(query_labels)"
   ]
  },
  {
   "source": [
    "Set `subset_classes=False` to retain all classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({0: 34, 3: 15, 2: 4, 1: 3, 4: 2, 5: 1, 6: 1}) 7\nCounter({0: 28, 3: 17, 2: 6, 1: 5, 6: 2, 4: 1, 5: 1}) 7\nCounter({0: 37, 3: 14, 2: 3, 1: 3, 6: 2, 4: 1, 5: 1}) 7\nCounter({0: 46, 3: 6, 2: 3, 4: 2, 1: 2, 5: 1, 6: 1}) 7\nCounter({0: 32, 3: 11, 2: 6, 1: 4, 4: 3, 6: 3, 5: 1}) 7\nCounter({0: 33, 3: 14, 1: 5, 2: 4, 4: 2, 6: 1, 5: 1}) 7\nCounter({0: 25, 3: 16, 2: 6, 1: 5, 4: 3, 6: 2, 5: 1}) 7\nCounter({0: 36, 3: 11, 2: 5, 6: 3, 4: 3, 1: 3, 5: 1}) 7\nCounter({0: 27, 3: 14, 2: 10, 1: 4, 5: 2, 4: 2, 6: 1}) 7\nCounter({0: 23, 3: 23, 2: 4, 1: 3, 6: 3, 4: 2, 5: 2}) 7\n"
     ]
    }
   ],
   "source": [
    "trainloader = AdaptiveNKShotLoader(dataset=dataset, device=torch.device('cpu'), max_support_size=64, subset_classes=False)\n",
    "\n",
    "for i in range(10):\n",
    "    support_labels, support_text, query_labels, query_text = next(trainloader)\n",
    "    print(Counter(support_labels), len(set(support_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({0: 41, 3: 11, 2: 5, 1: 2, 5: 1, 4: 1}) 6\nCounter({3: 26, 0: 19, 2: 7, 1: 6, 5: 2, 4: 2}) 6\nCounter({0: 11, 2: 6, 3: 1, 1: 1}) 4\nCounter({0: 12, 3: 9, 2: 3, 1: 3, 4: 1}) 5\nCounter({2: 23, 0: 17, 1: 7, 3: 1, 4: 1}) 5\nCounter({0: 36, 3: 11, 2: 8, 1: 4, 5: 1, 4: 1}) 6\nCounter({0: 37, 1: 19, 2: 7}) 3\nCounter({2: 19, 0: 18, 1: 3, 3: 1}) 4\nCounter({0: 38, 3: 9, 2: 7, 1: 5, 5: 1, 4: 1}) 6\nCounter({0: 47, 2: 8, 1: 4, 4: 1, 3: 1}) 5\n"
     ]
    }
   ],
   "source": [
    "trainloader = AdaptiveNKShotLoader(dataset=dataset, device=torch.device('cpu'), max_support_size=64, subset_classes=True)\n",
    "\n",
    "for i in range(10):\n",
    "    support_labels, support_text, query_labels, query_text = next(trainloader)\n",
    "    print(Counter(support_labels), len(set(support_labels)))"
   ]
  },
  {
   "source": [
    "Set `temp_map=False` to retain label definitions according to the dataset. \n",
    "\n",
    "Needs to be re-mapped to allow for generating one-hot vectors for loss computation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 1, 2]\n[0, 1, 2]\n[0, 1, 2, 3, 4, 5]\n[0, 1, 2, 3, 4]\n[0, 1, 2, 3]\n[0, 1, 2, 3]\n[0, 1, 2, 3, 4]\n[0, 1, 2]\n[0, 1, 2, 3, 4]\n[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "trainloader = AdaptiveNKShotLoader(dataset=dataset, device=torch.device('cpu'), max_support_size=64, temp_map=True)\n",
    "\n",
    "for i in range(10):\n",
    "    support_labels, support_text, query_labels, query_text = next(trainloader)\n",
    "    print(sorted(Counter(support_labels).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2, 4, 5, 6]\n",
      "[0, 1, 2]\n",
      "[0, 1, 2, 3, 5]\n",
      "[1, 2, 3, 4, 6]\n",
      "[1, 3, 5]\n",
      "[1, 2, 3, 4, 5, 6]\n",
      "[0, 1, 3, 4, 5, 6]\n",
      "[0, 1, 2, 3, 4, 6]\n",
      "[0, 1, 3, 4, 5, 6]\n",
      "[1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "trainloader = AdaptiveNKShotLoader(dataset=dataset, device=torch.device('cpu'), max_support_size=64, temp_map=False)\n",
    "\n",
    "for i in range(10):\n",
    "    support_labels, support_text, query_labels, query_text = next(trainloader)\n",
    "    print(sorted(Counter(support_labels).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2054,  999,  ...,    0,    0,    0],\n",
       "        [ 101, 2428, 1029,  ...,    0,    0,    0],\n",
       "        [ 101, 2025, 2172,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2339, 2064,  ...,    0,    0,    0],\n",
       "        [ 101, 2092, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 3524, 1010,  ...,    0,    0,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "dataset = unified.datasets['dailydialog']['train']\n",
    "\n",
    "trainloader = AdaptiveNKShotLoader(dataset=dataset, device=torch.device('cuda'), tokenizer=tokenizer, max_support_size=64, temp_map=True)\n",
    "for i in range(1000):\n",
    "    batch = next(trainloader)\n",
    "    support_labels, support_text, query_labels, query_text = batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd05807788318072d650ff7d3f12118168354b5d5c1b01788c0f8c7e03b8ca64544",
   "display_name": "Python 3.8.8 64-bit ('acts': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}