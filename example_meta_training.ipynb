{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd05807788318072d650ff7d3f12118168354b5d5c1b01788c0f8c7e03b8ca64544",
   "display_name": "Python 3.8.8 64-bit ('acts': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# An Episodic Training Example\n",
    "## Dataloaders setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.unified_emotion.unified_emotion import unified_emotion\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified = unified_emotion(\"./data/datasets/unified-dataset.jsonl\")\n",
    "\n",
    "unified.prep()"
   ]
  },
  {
   "source": [
    "## Model setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "from modules.mlp_clf import SF_CLF\n",
    "\n",
    "\n",
    "# Pre-trained model name (from Huggingface)\n",
    "bert_name = 'bert-base-uncased'\n",
    "# Pre-trained configuration from Huggingface\n",
    "bert_config = BertConfig.from_pretrained(bert_name)\n",
    "# Max to layer keep frozen. 11 keeps model frozen, -1 makes BERT totally trainable\n",
    "nu = 11\n",
    "\n",
    "# MLP hidden layers\n",
    "hidden_dims = [512, 256, 128]\n",
    "# Which activation to use. Currently either tanh or ReLU\n",
    "act_fn = 'ReLU'\n",
    "\n",
    "config = {'bert_name': bert_name, \n",
    "'bert_config': bert_config, \n",
    "'nu': nu,\n",
    "'hidden_dims': hidden_dims,\n",
    "'act_fn': act_fn\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MetaBert(\n  (encoder): BertSequence(\n    (model): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (6): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (7): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (8): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (9): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (10): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (11): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n  )\n  (mlp): MLP(\n    (mlp): Sequential(\n      (0): Linear(in_features=768, out_features=512, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=512, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=128, bias=True)\n      (5): ReLU()\n    )\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "from models.meta_bert import MetaBert\n",
    "\n",
    "model = MetaBert(config)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "# MAML"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import higher\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from data.utils.sampling import dataset_sampler\n",
    "\n",
    "task_optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "meta_optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "\n",
    "k = 4 # Number of shots\n",
    "n_inner = 5 # Number of inner loop updates\n",
    "n_outer = 10 # Number of outer loop updates before meta update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Enter the innerloop for grounded_emotions, N=2\n",
      "\tInner 0 | Loss=0.6940103769302368\n",
      "\tInner 1 | Loss=0.6929729580879211\n",
      "\tInner 2 | Loss=0.6936208605766296\n",
      "\tInner 3 | Loss=0.6938116550445557\n",
      "\tInner 4 | Loss=0.6934021711349487\n",
      "\tOuter   | Loss=0.6941459774971008\n",
      "Enter the innerloop for ssec, N=7\n",
      "\tInner 0 | Loss=1.9461575746536255\n",
      "\tInner 1 | Loss=1.9454056024551392\n",
      "\tInner 2 | Loss=1.948883056640625\n",
      "\tInner 3 | Loss=1.9445468187332153\n",
      "\tInner 4 | Loss=1.947567343711853\n",
      "\tOuter   | Loss=1.9467065334320068\n",
      "Enter the innerloop for tec, N=6\n",
      "\tInner 0 | Loss=1.7907174825668335\n",
      "\tInner 1 | Loss=1.7933412790298462\n",
      "\tInner 2 | Loss=1.7930980920791626\n",
      "\tInner 3 | Loss=1.7915347814559937\n",
      "\tInner 4 | Loss=1.793237566947937\n",
      "\tOuter   | Loss=1.7948771715164185\n",
      "Enter the innerloop for dailydialog, N=7\n",
      "\tInner 0 | Loss=1.9495872259140015\n",
      "\tInner 1 | Loss=1.9476367235183716\n",
      "\tInner 2 | Loss=1.9457828998565674\n",
      "\tInner 3 | Loss=1.9458239078521729\n",
      "\tInner 4 | Loss=1.9495996236801147\n",
      "\tOuter   | Loss=1.9501999616622925\n",
      "Enter the innerloop for tec, N=6\n",
      "\tInner 0 | Loss=1.7943154573440552\n",
      "\tInner 1 | Loss=1.7907806634902954\n",
      "\tInner 2 | Loss=1.798827052116394\n",
      "\tInner 3 | Loss=1.7911267280578613\n",
      "\tInner 4 | Loss=1.7925912141799927\n",
      "\tOuter   | Loss=1.7910528182983398\n",
      "Enter the innerloop for emoint, N=4\n",
      "\tInner 0 | Loss=1.3853826522827148\n",
      "\tInner 1 | Loss=1.3876991271972656\n",
      "\tInner 2 | Loss=1.389591932296753\n",
      "\tInner 3 | Loss=1.3906272649765015\n",
      "\tInner 4 | Loss=1.3883252143859863\n",
      "\tOuter   | Loss=1.3894367218017578\n",
      "Enter the innerloop for crowdflower, N=8\n",
      "\tInner 0 | Loss=2.0803093910217285\n",
      "\tInner 1 | Loss=2.087019443511963\n",
      "\tInner 2 | Loss=2.0861876010894775\n",
      "\tInner 3 | Loss=2.082165241241455\n",
      "\tInner 4 | Loss=2.08563494682312\n",
      "\tOuter   | Loss=2.0851054191589355\n",
      "Enter the innerloop for grounded_emotions, N=2\n",
      "\tInner 0 | Loss=0.6941285729408264\n",
      "\tInner 1 | Loss=0.6905555725097656\n",
      "\tInner 2 | Loss=0.6989089250564575\n",
      "\tInner 3 | Loss=0.6840670108795166\n",
      "\tInner 4 | Loss=0.6819598078727722\n",
      "\tOuter   | Loss=0.6947264671325684\n",
      "Enter the innerloop for crowdflower, N=8\n",
      "\tInner 0 | Loss=2.083256244659424\n",
      "\tInner 1 | Loss=2.0820236206054688\n",
      "\tInner 2 | Loss=2.0787529945373535\n",
      "\tInner 3 | Loss=2.0827910900115967\n",
      "\tInner 4 | Loss=2.0793862342834473\n",
      "\tOuter   | Loss=2.0830423831939697\n",
      "Enter the innerloop for dailydialog, N=7\n",
      "\tInner 0 | Loss=1.9733318090438843\n",
      "\tInner 1 | Loss=1.9695907831192017\n",
      "\tInner 2 | Loss=1.9675530195236206\n",
      "\tInner 3 | Loss=1.9637988805770874\n",
      "\tInner 4 | Loss=1.9655133485794067\n",
      "\tOuter   | Loss=1.9624699354171753\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_outer):\n",
    "    \n",
    "    # A single episode\n",
    "    # Set optimizer outside to 0\n",
    "    meta_optimizer.zero_grad()\n",
    "\n",
    "    # Sample a task\n",
    "    source_name = dataset_sampler(unified, sampling_method='sqrt')\n",
    "\n",
    "    # Get task dataloaders\n",
    "    trainloader, testloader = unified.get_dataloader(source_name, k=k, tokenizer=tokenizer, shuffle=True)\n",
    "\n",
    "    # Re-initialize the softmax layer\n",
    "    # Can be informed (e.g. ProtoMAML, LEOPARD, etc.)\n",
    "    n_classes = len(unified.label_map[source_name].keys())\n",
    "    clf_layer = SF_CLF(n_classes=n_classes, hidden_dims=hidden_dims)\n",
    "\n",
    "    print(f\"Enter the innerloop for {source_name}, N={n_classes}\")\n",
    "    with higher.innerloop_ctx(model, task_optimizer, copy_initial_weights=False, track_higher_grads=False) as (fmodel, diffopt):\n",
    "        \n",
    "        # MAML support sets\n",
    "        for ii in range(n_inner):\n",
    "            # Sample batch\n",
    "            batch = next(trainloader)\n",
    "            labels, text, attn_mask = batch\n",
    "            \n",
    "            # Use higher to perform inner loop updates\n",
    "            y = model(text, attn_mask)\n",
    "            logits = clf_layer(y)\n",
    "            inner_loss = lossfn(logits, labels)\n",
    "            diffopt.step(inner_loss)\n",
    "\n",
    "            print(f\"\\tInner {ii} | Loss={inner_loss.detach().tolist()}\", flush=True)\n",
    "        \n",
    "        # MAML query set\n",
    "        batch = next(trainloader)\n",
    "        labels, text, attn_mask = batch\n",
    "        \n",
    "        y = model(text, attn_mask)\n",
    "        logits = clf_layer(y)\n",
    "        outer_loss = lossfn(logits, labels)\n",
    "        outer_loss.backward()\n",
    "\n",
    "        print(f\"\\tOuter   | Loss={outer_loss.detach().tolist()}\", flush=True)\n",
    "\n",
    "    meta_optimizer.step()\n"
   ]
  },
  {
   "source": [
    "# ProtoFoMAML Mock-up"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from data.utils.sampling import dataset_sampler\n",
    "\n",
    "k = 4 # Number of shots\n",
    "n_inner = 5 # Number of inner loop updates\n",
    "n_outer = 10 # Number of outer loop updates before meta update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Set-up model + training\n",
    "\n",
    "model = MetaBert(config)\n",
    "\n",
    "meta_optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "lossfn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and embed\n",
    "# This is with M_init\n",
    "\n",
    "# Sample a task\n",
    "source_name = dataset_sampler(unified, sampling_method='sqrt')\n",
    "\n",
    "# Get task dataloaders\n",
    "trainloader, testloader = unified.get_dataloader(source_name, k=k, tokenizer=tokenizer, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# Generate separate model\n",
    "from copy import deepcopy \n",
    "\n",
    "# Copy M_init to task-specific model\n",
    "model_episode = deepcopy(model)\n",
    "model_episode.zero_grad()\n",
    "\n",
    "# Embed with init_model\n",
    "batch = next(trainloader)\n",
    "labels, text, attn_mask = batch\n",
    "\n",
    "y_init = model(text, attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# Generate prototypes and weight/biases from these\n",
    "import torch\n",
    "\n",
    "prototypes = torch.stack([torch.mean(y_init[labels == i], dim=0) for i, _ in enumerate(unified.label_map[source_name].keys())])\n",
    "\n",
    "W = 2 * prototypes\n",
    "b = -torch.norm(prototypes, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "# Set the weights and biases\n",
    "W_output, b_output = W.detach(),  b.detach()\n",
    "W_output.requires_grad, b_output.requires_grad = True, True\n",
    "\n",
    "output_optimizer = optim.SGD([W_output, b_output], lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5\n",
    "# Episodic training\n",
    "# Repeat G times\n",
    "import torch.nn.functional as F\n",
    "\n",
    "G = 5\n",
    "for _ in range(G):\n",
    "    # Load data\n",
    "    batch = next(trainloader)\n",
    "    labels, text, attn_mask = batch\n",
    "\n",
    "    # Embed, encode, classify and compute loss\n",
    "    y = model_episode(text, attn_mask)\n",
    "    logits = F.linear(y, W_output, b_output)\n",
    "    loss = lossfn(logits, labels)\n",
    "\n",
    "    # Backprop the output parameters\n",
    "    # Retrain graph for shared parameters\n",
    "    W_output.grad, b_output.grad = torch.autograd.grad(loss, [W_output, b_output], retain_graph=True)\n",
    "\n",
    "    # Backprop on shared parameters here    \n",
    "    updateable_episode_params = [param for param in model_episode.parameters() if param.requires_grad]\n",
    "    episode_grads = torch.autograd.grad(loss, updateable_episode_params)\n",
    "\n",
    "    for param, grad in zip(updateable_episode_params, episode_grads):\n",
    "        param.grad = grad\n",
    "\n",
    "    # Update the parameters\n",
    "    output_optimizer.step()\n",
    "    task_optimizer.step()\n",
    "\n",
    "    output_optimizer.zero_grad()\n",
    "    task_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6\n",
    "# Unify the graphs\n",
    "\n",
    "W_output = W + (W_output - W).detach()\n",
    "b_output = b + (b_output - b).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7\n",
    "# Evaluate on query\n",
    "batch = next(trainloader)\n",
    "labels, text, attn_mask = batch\n",
    "\n",
    "y = model_episode(text, attn_mask)\n",
    "logits = F.linear(y, W_output, b_output)\n",
    "loss = lossfn(logits, labels)\n",
    "\n",
    "# Backprop on task-specific parameters here    \n",
    "updateable_episode_params = [param for param in model_episode.parameters() if param.requires_grad]\n",
    "episode_grads = torch.autograd.grad(loss, updateable_episode_params, retain_graph=True)\n",
    "\n",
    "# Backprop on initial parameters here\n",
    "updateable_model_params = [param for param in model.parameters() if param.requires_grad]\n",
    "model_grads = torch.autograd.grad(loss, updateable_model_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8\n",
    "# Sum the gradients\n",
    "for param, g_episode, g_init in zip(updateable_model_params, episode_grads, model_grads):\n",
    "    if param.grad == None:\n",
    "        param.grad = g_episode + g_init\n",
    "    else:\n",
    "        param.grad += g_episode + g_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outerloop update\n",
    "# Update after a number of tasks\n",
    "meta_optimizer.step()\n",
    "meta_optimizer.zero_grad()"
   ]
  },
  {
   "source": [
    "## Same but with nn.Linear\n",
    "**TODO, ask Phillip how to init while part of compute graph**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-1c965f4fccf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Copy to clf layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munified\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "# Copy to clf layer\n",
    "clf = nn.Linear(model.out_dim, len(unified.label_map[source_name].keys()))\n",
    "clf.weight.copy_(W.detach())\n",
    "clf.bias.copy_(b.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = nn.Linear(model.out_dim, len(unified.label_map[source_name].keys()))\n",
    "with torch.no_grad():\n",
    "    clf.weight.copy_(W)\n",
    "    clf.bias.copy_(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-83c4ace297eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\acts\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m                 raise TypeError(\"cannot assign '{}' as parameter '{}' \"\n\u001b[0m\u001b[0;32m    969\u001b[0m                                 \u001b[1;34m\"(torch.nn.Parameter or None expected)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m                                 .format(torch.typename(value), name))\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "clf.weight = 2 * W + (clf.weight - 2 * W).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits to predictions\n",
    "probs = F.softmax(F.linear(y, W_output, b_output), dim=-1)\n",
    "preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "# Accuracy\n",
    "acc = (preds == labels).float().mean()\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat = torch.zeros(probs.size(-1), probs.size(-1))\n",
    "for i, j in zip(preds, labels):\n",
    "    conf_mat[i, j] += 1\n",
    "\n",
    "# Precision / Recall\n",
    "precision = torch.nan_to_num(torch.diagonal(conf_mat) / torch.sum(conf_mat, dim = 1))\n",
    "recall = torch.nan_to_num(torch.diagonal(conf_mat) / torch.sum(conf_mat, dim = 0))\n",
    "\n",
    "# F1\n",
    "F1 = torch.nan_to_num(2 * (precision * recall) / (precision + recall))\n",
    "\n",
    "# Macro agg \n",
    "F1_macro, F1_std = torch.mean(F1), torch.std(F1)\n",
    "\n",
    "# Micro agg\n"
   ]
  }
 ]
}