{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd05807788318072d650ff7d3f12118168354b5d5c1b01788c0f8c7e03b8ca64544",
   "display_name": "Python 3.8.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# An Episodic Training Example\n",
    "## Dataloaders setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.unified_emotion.unified_emotion import unified_emotion\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified = unified_emotion(\"./data/datasets/unified-dataset.jsonl\")\n",
    "\n",
    "unified.prep()"
   ]
  },
  {
   "source": [
    "## Model setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "from modules.mlp_clf import SF_CLF\n",
    "\n",
    "\n",
    "# Pre-trained model name (from Huggingface)\n",
    "bert_name = 'bert-base-uncased'\n",
    "# Pre-trained configuration from Huggingface\n",
    "bert_config = BertConfig.from_pretrained(bert_name)\n",
    "# Max to layer keep frozen. 11 keeps model frozen, -1 makes BERT totally trainable\n",
    "nu = 11\n",
    "\n",
    "# MLP hidden layers\n",
    "hidden_dims = [512, 256, 128]\n",
    "# Which activation to use. Currently either tanh or ReLU\n",
    "act_fn = 'ReLU'\n",
    "\n",
    "config = {'bert_name': bert_name, \n",
    "'bert_config': bert_config, \n",
    "'nu': nu,\n",
    "'hidden_dims': hidden_dims,\n",
    "'act_fn': act_fn\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MetaBert(\n  (encoder): BertSequence(\n    (model): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (6): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (7): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (8): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (9): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (10): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (11): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n  )\n  (mlp): MLP(\n    (mlp): Sequential(\n      (0): Linear(in_features=768, out_features=512, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=512, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=128, bias=True)\n      (5): ReLU()\n    )\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "from models.meta_bert import MetaBert\n",
    "\n",
    "model = MetaBert(config)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "## Training setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import higher\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from data.utils.sampling import dataset_sampler\n",
    "\n",
    "task_optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "meta_optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "\n",
    "k = 4 # Number of shots\n",
    "n_inner = 5 # Number of inner loop updates\n",
    "n_outer = 10 # Number of outer loop updates before meta update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Enter the innerloop for dailydialog, N=7\n",
      "\tInner 0 | Loss=1.949546217918396\n",
      "\tInner 1 | Loss=1.9495809078216553\n",
      "\tInner 2 | Loss=1.9483307600021362\n",
      "\tInner 3 | Loss=1.9500354528427124\n",
      "\tInner 4 | Loss=1.948388934135437\n",
      "\tOuter   | Loss=1.9478541612625122\n",
      "Enter the innerloop for tec, N=6\n",
      "\tInner 0 | Loss=1.795456051826477\n",
      "\tInner 1 | Loss=1.7922641038894653\n",
      "\tInner 2 | Loss=1.7948722839355469\n",
      "\tInner 3 | Loss=1.7933133840560913\n",
      "\tInner 4 | Loss=1.7916769981384277\n",
      "\tOuter   | Loss=1.7909350395202637\n",
      "Enter the innerloop for dailydialog, N=7\n",
      "\tInner 0 | Loss=1.9476946592330933\n",
      "\tInner 1 | Loss=1.9475170373916626\n",
      "\tInner 2 | Loss=1.9461147785186768\n",
      "\tInner 3 | Loss=1.9486711025238037\n",
      "\tInner 4 | Loss=1.9497442245483398\n",
      "\tOuter   | Loss=1.9504438638687134\n",
      "Enter the innerloop for dailydialog, N=7\n",
      "\tInner 0 | Loss=1.959351897239685\n",
      "\tInner 1 | Loss=1.9613603353500366\n",
      "\tInner 2 | Loss=1.954295039176941\n",
      "\tInner 3 | Loss=1.9514131546020508\n",
      "\tInner 4 | Loss=1.951809287071228\n",
      "\tOuter   | Loss=1.954148292541504\n",
      "Enter the innerloop for tales-emotion, N=7\n",
      "\tInner 0 | Loss=1.965147852897644\n",
      "\tInner 1 | Loss=1.9619776010513306\n",
      "\tInner 2 | Loss=1.9500806331634521\n",
      "\tInner 3 | Loss=1.972348928451538\n",
      "\tInner 4 | Loss=1.9589887857437134\n",
      "\tOuter   | Loss=1.947257161140442\n",
      "Enter the innerloop for crowdflower, N=8\n",
      "\tInner 0 | Loss=2.085301160812378\n",
      "\tInner 1 | Loss=2.087320327758789\n",
      "\tInner 2 | Loss=2.0896129608154297\n",
      "\tInner 3 | Loss=2.0852468013763428\n",
      "\tInner 4 | Loss=2.0881388187408447\n",
      "\tOuter   | Loss=2.0855519771575928\n",
      "Enter the innerloop for emoint, N=4\n",
      "\tInner 0 | Loss=1.382799744606018\n",
      "\tInner 1 | Loss=1.3926761150360107\n",
      "\tInner 2 | Loss=1.3864409923553467\n",
      "\tInner 3 | Loss=1.3895045518875122\n",
      "\tInner 4 | Loss=1.3939136266708374\n",
      "\tOuter   | Loss=1.3922247886657715\n",
      "Enter the innerloop for tales-emotion, N=7\n",
      "\tInner 0 | Loss=1.9614781141281128\n",
      "\tInner 1 | Loss=1.9543650150299072\n",
      "\tInner 2 | Loss=1.947234869003296\n",
      "\tInner 3 | Loss=1.9529201984405518\n",
      "\tInner 4 | Loss=1.9608020782470703\n",
      "\tOuter   | Loss=1.955055832862854\n",
      "Enter the innerloop for dailydialog, N=7\n",
      "\tInner 0 | Loss=1.9704244136810303\n",
      "\tInner 1 | Loss=1.96609628200531\n",
      "\tInner 2 | Loss=1.9674245119094849\n",
      "\tInner 3 | Loss=1.9573506116867065\n",
      "\tInner 4 | Loss=1.9620975255966187\n",
      "\tOuter   | Loss=1.9637477397918701\n",
      "Enter the innerloop for tec, N=6\n",
      "\tInner 0 | Loss=1.7942452430725098\n",
      "\tInner 1 | Loss=1.8021818399429321\n",
      "\tInner 2 | Loss=1.7993637323379517\n",
      "\tInner 3 | Loss=1.8085209131240845\n",
      "\tInner 4 | Loss=1.8057562112808228\n",
      "\tOuter   | Loss=1.8017581701278687\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_outer):\n",
    "    \n",
    "    # A single episode\n",
    "    # Set optimizer outside to 0\n",
    "    meta_optimizer.zero_grad()\n",
    "\n",
    "    # Sample a task\n",
    "    source_name = dataset_sampler(unified, sampling_method='sqrt')\n",
    "\n",
    "    # Get task dataloaders\n",
    "    trainloader, testloader = unified.get_dataloader(source_name, k=k, tokenizer=tokenizer, shuffle=True)\n",
    "\n",
    "    # Re-initialize the softmax layer\n",
    "    # Can be informed (e.g. ProtoMAML, LEOPARD, etc.)\n",
    "    n_classes = len(unified.label_map[source_name].keys())\n",
    "    clf_layer = SF_CLF(n_classes=n_classes, hidden_dims=hidden_dims)\n",
    "\n",
    "    print(f\"Enter the innerloop for {source_name}, N={n_classes}\")\n",
    "    with higher.innerloop_ctx(model, task_optimizer, copy_initial_weights=False) as (fmodel, diffopt):\n",
    "        \n",
    "        # MAML support sets\n",
    "        for ii in range(n_inner):\n",
    "            # Sample batch\n",
    "            batch = next(trainloader)\n",
    "            labels, text, attn_mask = batch\n",
    "            \n",
    "            # Use higher to perform inner loop updates\n",
    "            y = model(text, attn_mask)\n",
    "            logits = clf_layer(y)\n",
    "            inner_loss = lossfn(logits, labels)\n",
    "            diffopt.step(inner_loss)\n",
    "\n",
    "            print(f\"\\tInner {ii} | Loss={inner_loss.detach().tolist()}\", flush=True)\n",
    "        \n",
    "        # MAML query set\n",
    "        batch = next(trainloader)\n",
    "        labels, text, attn_mask = batch\n",
    "        \n",
    "        y = model(text, attn_mask)\n",
    "        logits = clf_layer(y)\n",
    "        outer_loss = lossfn(logits, labels)\n",
    "        outer_loss.backward()\n",
    "\n",
    "        print(f\"\\tOuter   | Loss={outer_loss.detach().tolist()}\", flush=True)\n",
    "\n",
    "    meta_optimizer.step()\n"
   ]
  }
 ]
}